{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmarks Path Computation\n",
    "In this notebook, we train emotion models using the CK+ dataset.\n",
    "- Read landmarks files and convert it to a normalized space.\n",
    "- TODO: PCA to avoid unnecessary landmarks\n",
    "- Based on the normalized landmarks, compute its patch regresssion.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to read and pre-process features from a file\n",
    "The dataset contains landmarks computed using AAM. These landmarks are stored as a sequence of (x \\n y \\n) floats dots. Each frame has its own landmark file. To achieve a normalized feature set, it is set the nose as the reference point and the distance between the eyes as the reference size. For each landmark, the final feature is its distance to the reference point divided by the reference size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Distances\n",
    "function readLandmarksFile(filename)\n",
    "    # println(\"Getting features from: \", filename)\n",
    "    \n",
    "    # Database parser\n",
    "    dots = map((x)->parse(x), split(readall(filename), ['\\n', ' '], limit=0, keep=false));\n",
    "    getLandmarkPoint(id) = [dots[id*2-1], dots[id*2]];\n",
    "    \n",
    "    # Reference Point: nose\n",
    "    referenceID = 34\n",
    "    referencePoint = getLandmarkPoint(referenceID)\n",
    "    \n",
    "    # Reference Size: distance between eyes\n",
    "    referenceSize = evaluate(Euclidean(), getLandmarkPoint(40), getLandmarkPoint(43))\n",
    "    if referenceSize < 1.0 error(\"Distance between eyes less than one.\") end\n",
    "    \n",
    "    # Normalized vector\n",
    "    normalizedVector = []\n",
    "    for id = 1:round(Int,(length(dots) / 2))\n",
    "        if id == referenceID; continue end\n",
    "        append!(normalizedVector, (referencePoint - getLandmarkPoint(id)) / referenceSize)\n",
    "    end\n",
    "    \n",
    "    return normalizedVector\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to perform patch regression over the landmarks\n",
    "Using the above function, all the landmarks from a video sequence are read and normalized. Then, for each landmark trajectory, it is computed its patch regression. The regression uses a second order polinomial function: a + bx + cx^2. The resulting vector contains the parameters [x0,xt,a,b,c] for each landmark patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns patch_xy[file][landmark*2]\n",
    "function getPatchFeatures(dir)\n",
    "    landmarks4frame = []\n",
    "    for f = readdir(\"$dir\")\n",
    "        push!(landmarks4frame, readLandmarksFile(\"$dir/$f\"))\n",
    "    end\n",
    "    return landmarks4frame\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# returns the (x y) patch for a given landmark\n",
    "function splitXY(patch, landmark)\n",
    "    x, y = Float64[], Float64[]\n",
    "    for file = 1:round(Int, length(patch))\n",
    "        push!(x, patch[file][landmark*2-1])\n",
    "        push!(y, patch[file][landmark*2])\n",
    "    end\n",
    "    return x, y\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# return abcd (cubic) params for a given (x, y) patch\n",
    "using LsqFit\n",
    "function cubicRegression(x, y)\n",
    "    model(x, p) = p[1] + p[2] * x + p[3] * x .^ 2 + p[4] * x .^ 3;\n",
    "    fit = curve_fit(model, x, y, [0.,0.,0.,0.]);\n",
    "    return fit.param\n",
    "end;\n",
    "function squareRegression(x, y)\n",
    "    model(x, p) = p[1] + p[2] * x + p[3] * x .^ 2;\n",
    "    fit = curve_fit(model, x, y, [0.,0.,0.]);\n",
    "    return fit.param\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Gadfly\n",
    "function drawCubicCurve(x, y, param, title=\"no title\")\n",
    "    plot(layer(x=x, y=y, Geom.point, order=1),\n",
    "         layer((x)->param[1] + param[2] * x + param[3] * x .^ 2 + param[4] * x .^ 3, minimum(x), maximum(x)),\n",
    "         Guide.title(title))\n",
    "end;\n",
    "function drawSquareCurve(x, y, param, title=\"no title\")\n",
    "    plot(layer(x=x, y=y, Geom.point, order=1),\n",
    "         layer((x)->param[1] + param[2] * x + param[3] * x .^ 2, minimum(x), maximum(x)),\n",
    "         Guide.title(title))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# performs regression for each landmark on a video clip\n",
    "# returns [x0, xt, a, b, c] * lenght(landmarks), i.e. SVM ready\n",
    "function landmarksRegressionParams(dir, debug=false)\n",
    "    abc = []\n",
    "    patch = getPatchFeatures(dir)\n",
    "    for landmark = 1:floor(Int, length(patch[1]) / 2)\n",
    "        x, y = splitXY(patch, landmark)\n",
    "        p = []\n",
    "        try\n",
    "            p = squareRegression(x, y)\n",
    "        catch\n",
    "            warn(\"Landmark $landmark: Skipping regression due to an awesome error while performing curve fitting\")\n",
    "            p = [0.,0.,0.]\n",
    "        finally\n",
    "            push!(abc, minimum(x))\n",
    "            push!(abc, maximum(x))\n",
    "            append!(abc, p)\n",
    "            if debug display(drawSquareCurve(x, y, p, \"landmark: $landmark\")) end            \n",
    "        end\n",
    "    end\n",
    "    return abc\n",
    "end;\n",
    "# p = landmarksRegressionParams(\"/home/data/ckplus/Landmarks/S100/001\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Gadfly\n",
    "function drawLandmarksCurves(param)     # TODO - a not so hardcoded revision\n",
    "    layers = Array{Gadfly.Layer,1}[]\n",
    "    for i in 1:floor(Int, length(param) / 5)\n",
    "        x0, xt, a, b, c = param[i*5-4], param[i*5-3], param[i*5-2], param[i*5-1], param[i*5]\n",
    "        push!(layers, layer((x)->a+b*x+c*x.^2, x0, xt))\n",
    "    end\n",
    "    plot( layers[1], layers[2], layers[3], layers[4], layers[5], layers[6], layers[7], layers[8], layers[9], layers[10], layers[11], layers[12], layers[13], layers[14], layers[15], layers[16], layers[17], layers[18], layers[19], layers[20], layers[21], layers[22], layers[23], layers[24], layers[25], layers[26], layers[27], layers[28], layers[29], layers[30], layers[31], layers[32], layers[33], layers[34], layers[35], layers[36], layers[37], layers[38], layers[39], layers[40], layers[41], layers[42], layers[43], layers[44], layers[45], layers[46], layers[47], layers[48], layers[49], layers[50], layers[51], layers[52], layers[53], layers[54], layers[55], layers[56], layers[57], layers[58], layers[59], layers[60], layers[61], layers[62], layers[63], layers[64], layers[65], layers[66], layers[67], Guide.title(\"Facial Landmarks Patchs\"))\n",
    "end;\n",
    "# drawLandmarksCurves(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to iterate over the CK+ dataset\n",
    "For each sample, read the landmarks and the emotion result files. Using the above functions, the landmarks are transformed to an array of regression parameters. To perform the SVM trainning, the trainning matrix is composed by the regression parameters, where each row represent a sample video. In the same manner, the result matrix (for trainning) is composed by the emotion ID for the respective sample in the trainning matrix. So, for SVM trainning there are two matrixes, MTrainning: samples x (landmarks * regression_params), MTResults: samples x 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the sample file containing Emotion or FACS codes\n",
    "function readResultFile(dir)\n",
    "    results = []\n",
    "    if !ispath(dir)\n",
    "        return results\n",
    "    end\n",
    "    for a in readdir(dir)\n",
    "        filename = \"$dir/$a\"\n",
    "        append!(results, map((x)->convert(Int, parse(x)), split(readall(filename), ['\\n', ' '], limit=0, keep=false)))\n",
    "    end\n",
    "    return(results)\n",
    "end;\n",
    "# readResultFile(\"/home/data/ckplus/Emotion/S005/001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loadTrainningSamples (generic function with 1 method)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterate over all samples to mount the trainning matrix for svm: X <samples> x <landmarks * regression_params>\n",
    "# returns also the path \"$subject/$sample\" referencing each sample in X\n",
    "function loadTrainningSamples(dir)\n",
    "    landmarksDir = \"$dir/Landmarks\"\n",
    "    X, Path = Matrix, []\n",
    "    first, c = true, 0\n",
    "    for subjectDir = readdir(landmarksDir), sampleDir = readdir(\"$landmarksDir/$subjectDir\")\n",
    "        c += 1\n",
    "        if (c % 100 == 0) println(\"$c\") end\n",
    "        row = []\n",
    "        try\n",
    "            row = landmarksRegressionParams(\"$landmarksDir/$subjectDir/$sampleDir\")\n",
    "        catch\n",
    "            warn(\"$c: Skipping due to some bizarre error while loading landmarks regression parameters.\")\n",
    "            continue\n",
    "        end\n",
    "        if !first\n",
    "            X = vcat(X, reshape(row, 1, size(row)[1]))\n",
    "        else\n",
    "            X = reshape(row, 1, size(row)[1])\n",
    "            first = false\n",
    "        end\n",
    "        push!(Path, \"$subjectDir/$sampleDir\")\n",
    "    end\n",
    "    return X, Path\n",
    "end\n",
    "# @time S, P = loadTrainningSamples(\"/home/data/ckplus\");\n",
    "# drawLandmarksCurves(S[100,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# todo: support facs\n",
    "function parseSVM_XY(dir, samples, paths, key, debug = false)\n",
    "    X, Y = Matrix(0,size(samples)[2]) , []\n",
    "    c = 0\n",
    "    for p in paths\n",
    "        c += 1\n",
    "        if debug \n",
    "            if c > 10 break end\n",
    "            println(\"--- $c: $p ---\")\n",
    "        end\n",
    "        results = readResultFile(\"$dir/$p\")\n",
    "        if length(results) > 0\n",
    "            for r in results\n",
    "                ry = (r == key)? 1: -1\n",
    "                X = vcat(X, samples[c,:])\n",
    "                push!(Y, ry)\n",
    "                if debug println(\"$c: $r -> $ry\") end\n",
    "            end\n",
    "        else\n",
    "            X = vcat(X, samples[c,:])\n",
    "            push!(Y, -1)\n",
    "            if debug println(\"$c: nothing -> -1\") end\n",
    "        end\n",
    "    end\n",
    "    return X, Y\n",
    "end\n",
    "# X, Y = parseSVM_XY(\"/home/data/ckplus/Emotion\", S, P, 6, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using RegERMs\n",
    "function trainCKPlus(dir, debug = false)\n",
    "    # landmarksDir = \"$dir/Landmarks\n",
    "    # emotionDir = \"$dir/Emotion\"\n",
    "    # facsDir =  \"$dir/FACS\"\n",
    "    \n",
    "    # Train Emotion\n",
    "    S, P = loadTrainningSamples());\n",
    "    \n",
    "end\n",
    "\n",
    "trainCKPlus(\"/home/data/ckplus\", true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = Matrix(0,size(S)[2])\n",
    "S[1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vcat(a, S[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "size(S)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.1",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
